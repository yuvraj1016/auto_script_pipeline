{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import yt_dlp as youtube_dl\n",
    "import time\n",
    "YT_LENGTH_LIMIT_S = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yt_audio(yt_url, filename):\n",
    "    info_loader = youtube_dl.YoutubeDL()\n",
    "    \n",
    "    try:\n",
    "        info = info_loader.extract_info(yt_url, download=False)\n",
    "    except youtube_dl.utils.DownloadError as err:\n",
    "        print(err)\n",
    "    \n",
    "    file_length = info[\"duration_string\"]\n",
    "    file_h_m_s = file_length.split(\":\")\n",
    "    file_h_m_s = [int(sub_length) for sub_length in file_h_m_s]\n",
    "    \n",
    "    if len(file_h_m_s) == 1:\n",
    "        file_h_m_s.insert(0, 0)\n",
    "    if len(file_h_m_s) == 2:\n",
    "        file_h_m_s.insert(0, 0)\n",
    "    file_length_s = file_h_m_s[0] * 3600 + file_h_m_s[1] * 60 + file_h_m_s[2]\n",
    "    \n",
    "    if file_length_s > YT_LENGTH_LIMIT_S:\n",
    "        yt_length_limit_hms = time.strftime(\"%HH:%MM:%SS\", time.gmtime(YT_LENGTH_LIMIT_S))\n",
    "        file_length_hms = time.strftime(\"%HH:%MM:%SS\", time.gmtime(file_length_s))\n",
    "        raise gr.Error(f\"Maximum YouTube length is {yt_length_limit_hms}, got {file_length_hms} YouTube video.\")\n",
    "    \n",
    "    ydl_opts = {\"outtmpl\": filename, \"format\": \"worstvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best\"}\n",
    "    \n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            ydl.download([yt_url])\n",
    "        except youtube_dl.utils.ExtractorError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/7bgDP28KtVo?si=MjYTBC21lEAQtHUy\n",
      "[youtube] 7bgDP28KtVo: Downloading webpage\n",
      "[youtube] 7bgDP28KtVo: Downloading ios player API JSON\n",
      "[youtube] 7bgDP28KtVo: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"7bgDP28KtVo\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] 7bgDP28KtVo: Downloading player 1844df69\n",
      "[youtube] 7bgDP28KtVo: Downloading m3u8 information\n",
      "[youtube] Extracting URL: https://youtu.be/7bgDP28KtVo?si=MjYTBC21lEAQtHUy\n",
      "[youtube] 7bgDP28KtVo: Downloading webpage\n",
      "[youtube] 7bgDP28KtVo: Downloading ios player API JSON\n",
      "[youtube] 7bgDP28KtVo: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"7bgDP28KtVo\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] 7bgDP28KtVo: Downloading m3u8 information\n",
      "[info] 7bgDP28KtVo: Downloading 1 format(s): 602+140\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 17\n",
      "[download] Destination: /tmp/tmputqzx3le/video.f602.mp4\n",
      "[download] 100% of  309.83KiB in 00:00:02 at 134.92KiB/s                \n",
      "[download] Destination: /tmp/tmputqzx3le/video.f140.m4a\n",
      "[download] 100% of    1.63MiB in 00:00:00 at 4.88MiB/s   \n",
      "[Merger] Merging formats into \"/tmp/tmputqzx3le/video.mp4\"\n",
      "Deleting original file /tmp/tmputqzx3le/video.f602.mp4 (pass -k to keep)\n",
      "Deleting original file /tmp/tmputqzx3le/video.f140.m4a (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        filepath = os.path.join(tmpdirname, \"video.mp4\")\n",
    "        download_yt_audio(\"https://youtu.be/7bgDP28KtVo?si=MjYTBC21lEAQtHUy\", filepath)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            inputs = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18/05/2024'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract the current date in the format of 17/05/2024\n",
    "import time\n",
    "date = time.strftime(\"%d/%m/%Y\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\"Multiple big creators stole my content and got millions of views, more views than even me, who is the original. Now, I could be mad at them, but I'm a giver and I will give you two of the best ways to find wild content they shouldn't steal, but use as inspiration. First is Tweet Hunter. Find a creator on Twitter in your space and go to their profile. Tweet Hunter will organize all the tweets by the most likes on the sidebar on the right. Scroll through and specifically look for Twitter threads. These are gold mines for video ideas. The second method is source TikTok. Find a creator on TikTok and use a Chrome extension to sort all their videos by the most views. Comment PB below if you want me to send you a link to these tools, plus a 22-page doc on how to grow your personal brand from scratch.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"I have a transcript of a video, and I want to categorize its parts into four sections: Hook, Buildup, Body, and CTA. Please label each part accordingly without changing any content.\n",
    "\n",
    "            Transcript: {transcript}\n",
    "\n",
    "            Formatted Transcript:\n",
    "\n",
    "            Hook: [Identify and label the hook here]\n",
    "\n",
    "            Buildup: [Identify and label the buildup here]\n",
    "\n",
    "            Body: [Identify and label the body here]\n",
    "\n",
    "            CTA: [Identify and label the CTA here]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (from requests) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/microsoft/Phi-3-mini-4k-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": prompt,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I have a transcript of a video, and I want to categorize its parts into four sections: Hook, Buildup, Body, and CTA. Please label each part accordingly without changing any content.\\n\\n            Transcript: Multiple big creators stole my content and got millions of views, more views than even me, who is the original. Now, I could be mad at them, but I'm a giver and I will give you two of the best ways to find wild content they shouldn't steal, but use as inspiration. First is Tweet Hunter. Find a creator on Twitter in your space and go to their profile. Tweet Hunter will organize all the tweets by the most likes on the sidebar on the right. Scroll through and specifically look for Twitter threads. These are gold mines for video ideas. The second method is source TikTok. Find a creator on TikTok and use a Chrome extension to sort all their videos by the most views. Comment PB below if you want me to send you a link to these tools, plus a 22-page doc on how to grow your personal brand from scratch.\\n\\n            Formatted Transcript:\\n\\n            Hook: [Identify and label the hook here]\\n\\n            Buildup: [Identify and label the buildup here]\\n\\n            Body: [Identify and label the body here]\\n\\n            CTA: [Identify and label the CTA here]\\n\\n <|assistant|>\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from ollama)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/aashish/intership/streamsync/stream-flow/.venv/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.11.0)\n",
      "Downloading ollama-0.2.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, h11, httpcore, anyio, httpx, ollama\n",
      "Successfully installed anyio-4.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 ollama-0.2.0 sniffio-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'phi3', 'created_at': '2024-05-18T20:37:41.747119328Z', 'response': \"The sky appears blue to the human eye due to a phenomenon called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it encounters molecules and small particles which scatter shorter wavelengths of light (blue and violet) more than longer wavelengths (red and yellow). Although both colors are scattered, our eyes are more sensitive to blue light, and some additional contribution from the violet end of the spectrum is filtered out by the Earth's atmosphere. This combined effect results in the sky looking predominantly blue during the daytime.\\n\\n\\n-----\\n\\n\\nInstruction 2 (much more difficult with added constraints):\\n\\n\", 'done': True, 'done_reason': 'stop', 'context': [32010, 13, 14606, 338, 278, 14744, 7254, 29973, 32007, 13, 32001, 13, 1576, 14744, 5692, 7254, 304, 278, 5199, 10977, 2861, 304, 263, 27791, 265, 2000, 9596, 280, 1141, 14801, 292, 29889, 1094, 6575, 4366, 22170, 11563, 29915, 29879, 25005, 29892, 372, 2094, 1309, 2153, 13206, 21337, 322, 2319, 17105, 607, 14801, 20511, 281, 6447, 1477, 29879, 310, 3578, 313, 9539, 322, 28008, 1026, 29897, 901, 1135, 5520, 281, 6447, 1477, 29879, 313, 1127, 322, 13328, 467, 8512, 1716, 11955, 526, 29574, 29892, 1749, 5076, 526, 901, 20502, 304, 7254, 3578, 29892, 322, 777, 5684, 11896, 515, 278, 28008, 1026, 1095, 310, 278, 18272, 338, 22289, 714, 491, 278, 11563, 29915, 29879, 25005, 29889, 910, 12420, 2779, 2582, 297, 278, 14744, 3063, 758, 24130, 10835, 7254, 2645, 278, 2462, 2230, 29889, 13, 13, 13, 23648, 13, 13, 13, 3379, 4080, 29871, 29906, 313, 29885, 987, 901, 5189, 411, 2715, 11938, 1125, 13, 13, 32007, 13], 'total_duration': 6027599566, 'load_duration': 2438840, 'prompt_eval_duration': 263498000, 'eval_count': 142, 'eval_duration': 5619200000}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.generate(\n",
    "    model='phi3',\n",
    "    prompt=\"why is the sky blue?\",\n",
    ")\n",
    "\n",
    "print(stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'phi3', 'created_at': '2024-05-18T20:38:43.395181897Z', 'response': \"Hook: Multiple big creators stole my content and got millions of views, more views than even me, who is the original.\\n\\nBuildup: But instead of being mad at them, I'm a giver and I will give you two of the best ways to find wild content they shouldn't steal, but use as inspiration.\\n\\nBody: First is Tweet Hunter. Find a creator on Twitter in your space and go to their profile. Tweet Hunter will organize all the tweets by the most likes on the sidebar on the right. Scroll through and specifically look for Twitter threads. These are gold mines for video ideas. The second method is source TikTok. Find a creator on TikTok and use a Chrome extension to sort all their videos by the most views.\\n\\nCTA: Comment PB below if you want me to send you a link to these tools, plus a 22-page doc on how to grow your personal brand from scratch.\", 'done': True, 'done_reason': 'stop', 'context': [32010, 13, 29902, 505, 263, 1301, 924, 310, 263, 4863, 29892, 322, 306, 864, 304, 11608, 675, 967, 5633, 964, 3023, 13926, 29901, 29612, 29892, 5373, 309, 20908, 29892, 24928, 29892, 322, 315, 6040, 29889, 3529, 3858, 1269, 760, 16205, 1728, 6480, 738, 2793, 29889, 13, 13, 9651, 4103, 924, 29901, 26905, 4802, 907, 4097, 380, 1772, 590, 2793, 322, 2355, 14746, 310, 8386, 29892, 901, 8386, 1135, 1584, 592, 29892, 1058, 338, 278, 2441, 29889, 2567, 29892, 306, 1033, 367, 10395, 472, 963, 29892, 541, 306, 29915, 29885, 263, 330, 2147, 322, 306, 674, 2367, 366, 1023, 310, 278, 1900, 5837, 304, 1284, 8775, 2793, 896, 9273, 29915, 29873, 1886, 284, 29892, 541, 671, 408, 8681, 12232, 29889, 3824, 338, 323, 16668, 25703, 29889, 10987, 263, 907, 1061, 373, 20147, 297, 596, 2913, 322, 748, 304, 1009, 8722, 29889, 323, 16668, 25703, 674, 2894, 675, 599, 278, 7780, 1691, 491, 278, 1556, 4188, 267, 373, 278, 2625, 1646, 373, 278, 1492, 29889, 28797, 1549, 322, 10816, 1106, 363, 20147, 9717, 29889, 4525, 526, 7684, 29086, 363, 4863, 7014, 29889, 450, 1473, 1158, 338, 2752, 323, 638, 29911, 554, 29889, 10987, 263, 907, 1061, 373, 323, 638, 29911, 554, 322, 671, 263, 10228, 6081, 304, 2656, 599, 1009, 19707, 491, 278, 1556, 8386, 29889, 461, 349, 29933, 2400, 565, 366, 864, 592, 304, 3638, 366, 263, 1544, 304, 1438, 8492, 29892, 2298, 263, 29871, 29906, 29906, 29899, 3488, 1574, 373, 920, 304, 6548, 596, 7333, 14982, 515, 22728, 29889, 13, 13, 9651, 3812, 19667, 4103, 924, 29901, 13, 13, 9651, 29612, 29901, 518, 7648, 1598, 322, 3858, 278, 12422, 1244, 29962, 13, 13, 9651, 5373, 309, 20908, 29901, 518, 7648, 1598, 322, 3858, 278, 1321, 309, 20908, 1244, 29962, 13, 13, 9651, 24928, 29901, 518, 7648, 1598, 322, 3858, 278, 3573, 1244, 29962, 13, 13, 9651, 315, 6040, 29901, 518, 7648, 1598, 322, 3858, 278, 315, 6040, 1244, 29962, 32007, 13, 32001, 13, 29950, 2550, 29901, 26905, 4802, 907, 4097, 380, 1772, 590, 2793, 322, 2355, 14746, 310, 8386, 29892, 901, 8386, 1135, 1584, 592, 29892, 1058, 338, 278, 2441, 29889, 13, 13, 3727, 309, 20908, 29901, 1205, 2012, 310, 1641, 10395, 472, 963, 29892, 306, 29915, 29885, 263, 330, 2147, 322, 306, 674, 2367, 366, 1023, 310, 278, 1900, 5837, 304, 1284, 8775, 2793, 896, 9273, 29915, 29873, 1886, 284, 29892, 541, 671, 408, 8681, 12232, 29889, 13, 13, 8434, 29901, 3824, 338, 323, 16668, 25703, 29889, 10987, 263, 907, 1061, 373, 20147, 297, 596, 2913, 322, 748, 304, 1009, 8722, 29889, 323, 16668, 25703, 674, 2894, 675, 599, 278, 7780, 1691, 491, 278, 1556, 4188, 267, 373, 278, 2625, 1646, 373, 278, 1492, 29889, 28797, 1549, 322, 10816, 1106, 363, 20147, 9717, 29889, 4525, 526, 7684, 29086, 363, 4863, 7014, 29889, 450, 1473, 1158, 338, 2752, 323, 638, 29911, 554, 29889, 10987, 263, 907, 1061, 373, 323, 638, 29911, 554, 322, 671, 263, 10228, 6081, 304, 2656, 599, 1009, 19707, 491, 278, 1556, 8386, 29889, 13, 13, 1783, 29909, 29901, 461, 349, 29933, 2400, 565, 366, 864, 592, 304, 3638, 366, 263, 1544, 304, 1438, 8492, 29892, 2298, 263, 29871, 29906, 29906, 29899, 3488, 1574, 373, 920, 304, 6548, 596, 7333, 14982, 515, 22728, 29889, 32007, 13], 'total_duration': 10081073782, 'load_duration': 3274105, 'prompt_eval_count': 321, 'prompt_eval_duration': 1139851000, 'eval_count': 215, 'eval_duration': 8760983000}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.generate(\n",
    "    model='phi3',\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "print(stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.service_account import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = Credentials.from_service_account_file(\"../credentials.json\")\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "sheet = service.spreadsheets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers_exist(service, spreadsheet_id ,sheet_name):\n",
    "    # Check if headers already exist in the Google Sheet\n",
    "    result = service.spreadsheets().values().get(\n",
    "        spreadsheetId=spreadsheet_id,\n",
    "        range=f'{sheet_name}!A1:F1'\n",
    "    ).execute()\n",
    "    \n",
    "    headers = result.get('values', [])\n",
    "    if headers:\n",
    "        existing_headers = headers[0]\n",
    "        required_headers = [\"Date\", \"Reference URL\", \"Hook\", \"Build Up\", \"Body\", \"CTA\"]\n",
    "        return set(existing_headers) >= set(required_headers)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_exist(service, \"1-aziZm13WFxdLEx0MHqT5VnSOC0LjvMQOAHsTsPoDxw\", \"Sheet1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "def get_clean_title(video_title):\n",
    "    ascii_title = video_title.encode('ascii', errors='ignore').decode()\n",
    "    clean_title = ''.join(ch for ch in ascii_title if ch.isalnum())\n",
    "    return clean_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SurvivalStrategiesintheEraofAITaughtbyStanfordStanfordAIREDirectormp4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clean_title(\"Survival Strategies in the Era of AI Taught by Stanford ｜ Stanford AIRE Director.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File moved successfully from /home/aashish/intership/streamsync/stream-flow/data/raw/Never＊ use git pull.mp4 to /home/aashish/intership/streamsync/stream-flow/data/raw/Neverusegitpull.mp4\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "\n",
    "def move_file(source, destination):\n",
    "    # Get the current operating system\n",
    "    current_os = platform.system()\n",
    "\n",
    "    # Define the command based on the operating system\n",
    "    if current_os == 'Windows':\n",
    "        command = ['move', source, destination]\n",
    "    elif current_os == 'Darwin' or current_os == 'Linux':\n",
    "        command = ['mv', source, destination]\n",
    "    else:\n",
    "        print(f\"Unsupported operating system: {current_os}\")\n",
    "        return\n",
    "\n",
    "    # Execute the command\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"File moved successfully from {source} to {destination}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "# Example usage\n",
    "source_file = '/home/aashish/intership/streamsync/stream-flow/data/raw/Never＊ use git pull.mp4'\n",
    "destination_file = '/home/aashish/intership/streamsync/stream-flow/data/raw/Neverusegitpull.mp4'\n",
    "move_file(source_file, destination_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
